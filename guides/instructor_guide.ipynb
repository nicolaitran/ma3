{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/instructor_header.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://medium.com/thoughts-on-machine-learning\">Thoughts on Machine Learning</a>'s \"<i><a href=\"https://medium.com/thoughts-on-machine-learning/drop-langchain-instructor-is-all-you-need-for-your-llm-based-applications-aed13e9b908b\">Drop LangChain, Instructor Is All You Need For Your LLM-Based Applications\"</a><br>by FS Ndzomga</i>. Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "***\n",
    "Sources: <br>\n",
    "- [Drop LangChain, Instructor Is All You Need For Your LLM-Based Applications (Medium)](https://medium.com/thoughts-on-machine-learning/drop-langchain-instructor-is-all-you-need-for-your-llm-based-applications-aed13e9b908b)\n",
    "\n",
    "\n",
    "# Instructor\n",
    "\n",
    "[Instructor](https://python.useinstructor.com/#getting-started) is a python package that makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including Mistral/Mixtral, Ollama, and llama-cpp-python - and WatsonX.ai models as we are using in this course. It stands out for its simplicity, transparency, and user-centric design, built on top of Pydantic. Instructor helps you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| Simple API with Full Prompt Control | Instructor provides a straightforward API that gives you complete ownership and control over your prompts. This allows for fine-tuned customization and optimization of your LLM interactions. |\n",
    "| Multi-Language Support | Simplify structured data extraction from LLMs with type hints and validation. Supports Python, TypeScript, Ruby, Go, Elixir, and Rust. |\n",
    "| Reasking and Validation | Automatically reask the model when validation fails, ensuring high-quality outputs. Leverage Pydantic's validation for robust error handling. |\n",
    "| Streaming Support | Stream partial results and iterables with ease, allowing for real-time processing and improved responsiveness in your applications. |\n",
    "| Powered by Type Hints | Leverage Pydantic for schema validation, prompting control, less code, and IDE integration. |\n",
    "| Simplified LLM Interactions | Support for OpenAI, Anthropic, Google, Vertex AI, Mistral/Mixtral, Ollama, llama-cpp-python, Cohere, LiteLLM. |\n",
    "\n",
    "Simply put, we can use Instructor to extract structured data from LLMs, instead of just plain test. In practically all cases, we want more than just a text dump from an LLM, and postprocessing LLM outputs can be a tedious, error-prone task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM\n",
    "\n",
    "One of the disadvantages of working with different LLM vendors (Azure AI, OpenAI, Anthropic, WatsonX etc.) is that they all have different API schemas. This means that we often have to build platform-specificer adapters if we are working with models from multiple places. [LiteLLM](https://docs.litellm.ai/) is an open source package that enable us to call [100+ LLMs from 56 providers](https://docs.litellm.ai/docs/providers) using the standard OpenAI Input/Output format. Behind the scenes, LiteLLM translates inputs to any provider's completion, embedding, and image_generation endpoints, and they ensures that we get a response that follows the OpenAI API schema back. \n",
    "\n",
    "You can read exactly how the [API works for WatsonX.ai here](https://docs.litellm.ai/docs/providers/watsonx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it together\n",
    "\n",
    "With LiteLLM we can initialize the WatsonX.ai models and then use Instructor to extract structured data from the LLMs. This is a powerful combination that allows us to work with multiple LLM vendors without having to worry about the differences in their APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libraries\n",
    "from typing import TypeVar, Literal, Any\n",
    "\n",
    "# litellm libraries\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from instructor import Mode, from_litellm\n",
    "\n",
    "# misc libraries\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading our WatsonX.ai credentials again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call a model from WatsonX.ai with LiteLLM first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call WATSONX `/text/chat` endpoint - supports function calling\n",
    "response = completion(\n",
    "  model=\"watsonx/meta-llama/llama-3-1-8b-instruct\",\n",
    "  messages=[{ \"content\": \"what is your favorite colour?\",\"role\": \"user\"}],\n",
    "  project_id=WX_PROJECT_ID,\n",
    "  api_key=WX_API_KEY,\n",
    "  base_url=WX_API_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-c9f020acd9aec8b0d6b76ac9256bb40d', created=1743361581, model='watsonx/meta-llama/llama-3-1-8b-instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I don't have personal preferences, including favorite colors. I'm designed to provide information and assist with tasks, but I don't have personal experiences or emotions. However, I can provide information about colors, color theory, and the psychological effects of different colors if you're interested.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=57, prompt_tokens=41, total_tokens=98, completion_tokens_details=None, prompt_tokens_details=None), model_id='meta-llama/llama-3-1-8b-instruct', model_version='3.1.0', created_at='2025-03-30T19:06:22.128Z', system={'warnings': [{'message': 'This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL.', 'id': 'disclaimer_warning', 'more_info': 'https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx'}, {'message': \"Model 'meta-llama/llama-3-1-8b-instruct' is in deprecated state from 2025-01-22. It will be in withdrawn state from 2025-05-30. IDs of alternative models: llama-3-2-11b-vision-instruct.\", 'id': 'deprecation_warning', 'more_info': 'https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp'}, {'message': \"The value of 'max_tokens' for this model was set to value 1024\", 'id': 'unspecified_max_token', 'additional_properties': {'limit': 0, 'new_value': 1024, 'parameter': 'max_tokens', 'value': 0}}]})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have personal preferences, including favorite colors. I'm a neutral AI assistant, and I don't have the capacity to experience emotions or have personal opinions. However, I can tell you about different colors, color theories, and even provide you with information on color palettes if you're interested.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Call WATSONX `/text/generation` endpoint - not all models support /chat route. \n",
    "response = completion(\n",
    "  model=\"watsonx/ibm/granite-3-2-8b-instruct\",\n",
    "  messages=[{ \"content\": \"Write a haiku about the singularity\",\"role\": \"user\"}],\n",
    "  project_id=WX_PROJECT_ID,\n",
    "  api_key=WX_API_KEY,\n",
    "  base_url=WX_API_URL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machines awake,\n",
      "   Binary dreams take human shape,\n",
      "   Singularity's birth.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's see how we can use `instructor` to pair with this neat interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.drop_params = True  # watsonx.ai doesn't support `json_mode`\n",
    "client = from_litellm(completion, mode=Mode.JSON)  # create an instructor client from litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a so-called `response_model`. This is a Pydantic model that defines the structure of the data we want to extract from the LLM. This is done using `pydantic` - another really great library for data validation and settings management. Pydantic is used by `instructor` to validate the data we get back from the LLM, and it also helps us to define the structure of the data we want to extract.\n",
    "\n",
    "Consider the example `Response` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a response model\n",
    "class Response(BaseModel): # <--- BaseModel is a Pydantic class\n",
    "\n",
    "    # ask the LLM to return a short reasoning - Remember how reasoning can help LLMs?\n",
    "    reasoning : str = Field(description=\"The short reasoning behind the answer\")\n",
    "    # ask the LLM to return the answer as a separate field\n",
    "    answer : float = Field(description=\"Your answer to the question as a float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are asking the LLM for two separate outputs:\n",
    "\n",
    "1. Reasoning of type `str` with a description added to give the LLM more context.\n",
    "2. The answer of type `str`, also with a description added to give the LLM more context.\n",
    "\n",
    "If we wanted to, for example, extract a reasoning an a float score, we could have done something like this:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning: str = Field(description=\"The reasoning behind the answer\")\n",
    "    score: float = Field(description=\"The score of the answer\")\n",
    "```\n",
    "\n",
    "We could even create nested models, like so:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Reasoning(BaseModel):\n",
    "    reasoning: str = Field(description=\"The reasoning behind the answer\")\n",
    "    score: float = Field(description=\"The score of the answer\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning: Reasoning = Field(description=\"The reasoning behind the answer\")\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "```\n",
    "\n",
    "Now, it should be noted that if we create more complex models, we might run into issues with smaller models - and even some bigger ones. Effectively, putting the answer into a response model can be considered an additional task we are asking the LLM to perform. Hence, we generally want to keep the response models as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we then use the response model we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a prompt\n",
    "prompt = \"\"\"You are a cat expert. Answer the following question about cats:\n",
    "Q: What is the average lifespan of a cat?\n",
    "Provide your answer as an object of Response\"\"\" # <-- We ask the model to return the answer as an object of Response\n",
    "\n",
    "# make a request to the LLM\n",
    "response = client.chat.completions.create( # <- Use the client we just created\n",
    "            model=\"watsonx/ibm/granite-3-2-8b-instruct\", # <--- model name from watsonx.ai\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,  # <- Our prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=WX_PROJECT_ID, # <- Our credentials\n",
    "            apikey=WX_API_KEY,\n",
    "            api_base=WX_API_URL,\n",
    "            response_model=Response, # <- Inform the LLM of the response model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(reasoning='The average lifespan of a cat is estimated by various sources, including the American Veterinary Medical Association and the American Society for the Prevention of Cruelty to Animals. These sources suggest that the average indoor cat lives between 13 and 17 years, while an outdoor cat typically lives between 3 to 5 years due to various dangers.', answer=15.5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average lifespan of a cat is estimated by various sources, including the American Veterinary Medical Association and the American Society for the Prevention of Cruelty to Animals. These sources suggest that the average indoor cat lives between 13 and 17 years, while an outdoor cat typically lives between 3 to 5 years due to various dangers.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.reasoning # <- Access the reasoning field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.answer # <- Access the answer field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going further, if we want the model to **only** be able to choose one of *n* answers, we can use the type `Literal`. This is a type hint that allows us to specify that the value of a field must be one of a set of literal values. For example, if we want the model to only be able to choose between \"Yes\" and \"No\", we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(answer='No')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a response model\n",
    "class Response(BaseModel): # <--- BaseModel is a Pydantic class\n",
    "\n",
    "    # ask the LLM to return a short reasoning - Remember how reasoning can help LLMs?\n",
    "    answer : Literal[\"Yes\", \"No\"] = Field(description=\"Your answer to the question\")\n",
    "\n",
    "prompt = \"\"\"You are a cat expert. Answer the following question about cats:\n",
    "\n",
    "Q: Is it true that cats have nine lives?\n",
    "\n",
    "Provide your answer as an object of Response\"\"\" # <-- We ask the model to return the answer as an object of Response\n",
    "\n",
    "# make a request to the LLM\n",
    "response = client.chat.completions.create( # <- Use the client we just created\n",
    "            model=\"watsonx/ibm/granite-3-2-8b-instruct\", # <--- model name from watsonx.ai\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,  # <- Our prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=WX_PROJECT_ID, # <- Our credentials\n",
    "            apikey=WX_API_KEY,\n",
    "            api_base=WX_API_URL,\n",
    "            response_model=Response, # <- Inform the LLM of the response model\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, right?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we don't want to define response models for every call we make to an LLM? `pydantic` (and therefore `instructor`) supports *dynamic* response models, via the `create_model` function. \n",
    "\n",
    "We can use that like shown below. Note that we have to define the type (i.e. str, int, float or bool) of each response field and add a `Field` object as well. The Field object can be used to define default values, add descriptions for the LLM etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_model = create_model(\n",
    "    \"MyResponseModel\", \n",
    "    reasoning=(str, Field(description=\"The short reasoning behind the answer\")),\n",
    "    answer=(str, Field(description=\"Your answer to the question\")),\n",
    "    __base__=BaseModel\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyResponseModel(reasoning='what the LLM would reason about', answer='what the LLM would answer')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model(reasoning=\"what the LLM would reason about\", answer=\"what the LLM would answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our life even easier, here is a class - `LLMCaller` that will do everything we just did for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseResponse(BaseModel):\n",
    "    \"\"\"A default response model that defines a single \n",
    "    field `answer` to store the response from the LLM.\n",
    "    We will use this when there is no need to create\n",
    "    a custom response model.\"\"\"\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define a type variable for the response model\n",
    "# this you can ignore for now - it is just for type hinting\n",
    "ResponseType = TypeVar('ResponseType', bound=BaseModel)\n",
    "\n",
    "\n",
    "class LLMCaller:\n",
    "    \"\"\" A class to interact with an LLM  using the LiteLLM and Instructor\n",
    "    libraries. This class is designed to simplify the process of sending\n",
    "    prompts to an LLM and receiving structured responses. \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, project_id: str, api_url: str, model_id: str, params: dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the LLMCaller instance with the necessary credentials and configuration.\n",
    "\n",
    "        Args:\n",
    "            api_key (str): The API key for authenticating with the LLM service.\n",
    "            project_id (str): The project ID associated with the LLM service.\n",
    "            api_url (str): The base URL for the LLM service API.\n",
    "            model_id (str): The identifier of the specific LLM model to use.\n",
    "            params (dict[str, Any]): Additional parameters to configure the LLM's behavior.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.project_id = project_id\n",
    "        self.api_url = api_url\n",
    "        self.model_id = model_id\n",
    "        self.params = params\n",
    "\n",
    "        # Boilerplate: Configure LiteLLM to drop unsupported parameters for Watsonx.ai\n",
    "        litellm.drop_params = True\n",
    "        # Boilerplate: Create an Instructor client for pydantic-based interactions with the LLM\n",
    "        self.client = from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "    def create_response_model(self, title: str, fields: dict) -> ResponseType:\n",
    "        \"\"\" Dynamically creates a Pydantic response model for the LLM's output.\n",
    "        Args:\n",
    "            title (str): The name of the response model.\n",
    "            fields (dict): A dictionary defining the fields of the response model.\n",
    "                           Keys are field names, and values are tuples of (type, Field).\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: A dynamically created Pydantic model class.\n",
    "        \"\"\"\n",
    "        return create_model(title, **fields, __base__=BaseResponse)\n",
    "\n",
    "    def invoke(self, prompt: str, response_model: ResponseType = BaseResponse, **kwargs) -> ResponseType:\n",
    "        \"\"\" Sends a prompt to the LLM and retrieves a structured response.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to send to the LLM.\n",
    "            response_model (ResponseType): The Pydantic model to structure the LLM's response.\n",
    "                                           Defaults to BaseResponse.\n",
    "            **kwargs: Additional arguments to pass to the LLM client.\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: The structured response from the LLM, parsed into the specified response model.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt + \"\\n\\n\" + f\"Provide your answer as an object of {type(response_model)}\",\n",
    "                    # notice how we hardcode instructions on the responde model type for the llm\n",
    "                    # so we don't have to repeat it in the prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            response_model=response_model,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMCaller(\n",
    "    api_key=WX_API_KEY,  # <- Our credentials\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    api_url=WX_API_URL,\n",
    "    model_id=\"watsonx/meta-llama/llama-3-3-70b-instruct\",  # <- model name from watsonx.ai\n",
    "    params={GenParams.MAX_NEW_TOKENS: 100}  # <- additional parameters for the LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseResponse(answer='A good name for a bee could be Buzz or Honey.')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is a good name for a bee?\")  # call with no response model - meaning we will use the default one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to feed in our dynamic response model, we can do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good name for a bee could be Buzzina\n",
      "The name Buzzina is a play on the word 'buzz', which is the sound bees make when they fly. It's a cute and memorable name that suits a busy and energetic bee\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\n",
    "    prompt=\"What is a good name for a bee? Think carefully.\", \n",
    "    response_model=model.create_response_model(  # create a response model dynamically\n",
    "        \"BeeName\", \n",
    "        {\n",
    "            \"reasoning\": (str, Field(...)),\n",
    "            \"bee_name\": (str, Field\n",
    "                (\n",
    "                    ...,\n",
    "                    description=\"The name of the bee.\"\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.answer)\n",
    "print(response.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you see how valuable the combination of `instructor` and `litellm` can be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
